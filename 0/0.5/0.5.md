# 0.5. Jorge Justo

# ¿Cuál es mi experiencia en desarrollo de software?
Mi experiencia va desde el inicio de la carrera de Ing de Sistemas, donde aprendi a programar y elaborar aplicativos en cursos como Ingenieria de Software y ahora en Seminario de Investigacion. Asi mismo, actualmente trabajo en una compania donde me encargo del development mobile para la empresa utilizando plataformas low-code.

# ¿Qué tecnologías conozco o domino?
Al momento tengo conocimientos en lenguajes como C, python y Java. Aparte, tengo conocimientos en SAP, Excel, Mendix (plataformas low-code), etc.

# ¿Cuál es mi expectativa del curso?
Aprender poco a poco a  comprender el dise;o y monitorear el desarrollo de una arquitectura correcta al momento de desarrollar una aplicaciones

# ¿Cómo me veo en 10 años?
Me gustaria ser un ingeniero de software o developer que trabaje con un grupo de manera iterativa y en equipo. Posiblemente en alguna empresa de redes sociales o entretenimiento.


# **Tema Individual: Microfrontends**

Hoy en día, las aplicaciones web son cada vez más complejas, y la necesidad de mantenerlas ágiles, escalables y fáciles de actualizar ha llevado a la adopción de microfrontends. Esta arquitectura se inspira en el concepto de microservicios del backend, pero aplicada al frontend. Es decir, los microfrontends dividen una aplicación grande en pequeños módulos independientes, permitiendo que varios equipos trabajen simultáneamente sin interferir entre sí. De este modo, cada módulo se puede desarrollar, actualizar e incluso desplegar por separado sin que el usuario lo note.

## **Aplicacion de Microfrontends en la Vida Real**

- Tomando por ejemplo un app de una tienda. El módulo del carrito de compras, la búsqueda de productos y las recomendaciones personalizadas podrían ser desarrollados como microfrontends independientes. Si un equipo necesita actualizar la funcionalidad del carrito, puede hacerlo sin preocuparse de romper el buscador o las recomendaciones.

Un caso similar sucede en las plataformas SaaS (como un sistema ERP). Cada área, ya sea finanzas, ventas o recursos humanos, se gestiona de forma autónoma. Así, cada equipo puede enfocarse en su módulo y actualizarlo sin afectar el rendimiento general de la plataforma.

## **Importancia**

- Autonomía para los Equipos: Cada equipo puede trabajar de manera independiente, utilizando diferentes tecnologías y metodologías, lo que agiliza los tiempos de desarrollo.
- Actualizaciones Sin Riesgos: Como cada módulo es autónomo, se pueden implementar mejoras o resolver errores sin afectar otros componentes de la aplicación.
- Escalabilidad Modular: Añadir nuevas funcionalidades es mucho más fácil porque cada módulo puede crecer por separado.
- Mejor Rendimiento: Como se carga solo lo necesario, la aplicación puede ser más rápida, especialmente en dispositivos móviles o con conexiones lentas.
  
## **Claves para Implementar Microfrontends con Éxito**

 - Comunicación y Sincronización entre Módulos: Aunque cada microfrontend es independiente, necesitan comunicarse entre sí para garantizar una experiencia coherente. Esto se suele resolver mediante eventos personalizados o APIs. Si, por ejemplo, el usuario añade un producto al carrito desde el módulo de búsqueda, esa acción debe reflejarse sin problemas en el módulo del carrito.

Por otro lado, si se requiere compartir información global (como el estado de sesión del usuario), pueden utilizarse herramientas como Redux o Context API para gestionar un estado común.

## **Herramientas Comunes en Microfrontends**

Para integrar los diferentes módulos dentro de una sola aplicación se utilizan frameworks como Single-SPA o técnicas como Webpack Module Federation. Estas herramientas facilitan la carga dinámica de cada microfrontend y aseguran que trabajen juntos sin conflictos.

**En mas detalle**:

- Module Federation es una característica de **Webpack 5** que permite a las aplicaciones web compartir módulos y dependencias de JavaScript entre sí en tiempo de ejecución de forma dinámica sin la necesidad de incluirlos en la compilación
- Single-spa es un framework diseñado para implementar microfrontends. Permite combinar varias aplicaciones frontend independientes en una sola página, donde cada microfrontend puede ser desarrollado, desplegado y mantenido de forma autónoma. Single-spa gestiona la carga y descarga de estos microfrontends según las rutas o condiciones, facilitando la integración de diferentes tecnologías (como React, Angular o Vue) en una misma aplicación sin problemas. Es útil para proyectos a gran escala, ya que promueve una arquitectura modular y escalable.
  
  Cómo Empezar: Instalación y Configuración
  - Paso 1: Asegúrate de tener Node.js y npm instalados, ya que estas herramientas gestionan las dependencias.
    Node se puede instalar desde la pagina https://nodejs.org/en

    Para revisar su correcta instalacion, se puede verificar con comandos como:
```
node -v  
npm -v   

```
    
  - Paso 2: Configura los microfrontends con Webpack o Single-SPA, según tus necesidades.
    
  - Paso 3: Despliega los módulos por separado. Esto permite que cada uno pueda ser actualizado sin tener que tocar la aplicación completa.
 

Al desplegar los modulos por separado, a veces se tiene que decidir entre el uso de un solo framework, o multiples. En muchos casos de implementacion de Microfrontends, se opta por el uso de multiples aplicativos del mismas librerias, como React por ejemplo. En otros casos, se puede tener instancias o proyectos donde se prefiere el uso de un modulo en React, otro en Vue, y otro en Angular, etc.

Estas decisiones pueden tener impacto en el desarrollo del app. 

**Ventajas de Usar un solo framework/libreria como React en Todos los Microfrontends**

- Consistencia en la UI y UX: Al usar React en todos los microfrontends, se logra una experiencia uniforme para los usuarios y una apariencia coherente entre diferentes módulos.

- Menor curva de aprendizaje: Si todos los equipos están familiarizados con React, no se necesitará capacitar a los desarrolladores en múltiples frameworks o bibliotecas.

- Reutilización de componentes: Puedes compartir componentes React entre microfrontends mediante librerías internas o repositorios compartidos.
  
- Facilita la integración: Al trabajar en un ecosistema común, es más fácil integrar los microfrontends y resolver problemas de interoperabilidad.
  
- Soporte para herramientas modernas: React tiene un fuerte ecosistema de herramientas (como Webpack, React Router, y Redux), lo que facilita la creación de aplicaciones modulares.

**Desventajas de un solo framework/libreria como React**

- Dependencia tecnológica: Usar exclusivamente una sola libreria puede limitar la flexibilidad y dificultar la adopción de nuevas tecnologías en el futuro.
- Desempeño: Si los microfrontends crecen mucho y cargan varias versiones de React (por ejemplo, si cada microfrontend necesita versiones distintas), puede aumentar el tamaño del bundle, afectando el tiempo de carga.
- Encapsulamiento limitado: Al usar el mismo framework para todos los microfrontends, puede ser más difícil aislar completamente las dependencias entre ellos, incrementando el riesgo de conflictos.
- mPérdida de especialización: Cada microfrontend tiene requisitos específicos y podría beneficiarse de tecnologías más especializadas, como Svelte (por eficiencia) o Angular (para aplicaciones empresariales complejas).

Al final del dia, depende del equipo de desarrollo decidir que enfoque beneficia mas al caso de uso.

    
## **Desafíos y Obstáculos Comunes**
 - Aunque esta arquitectura ofrece muchos beneficios, no es perfecta. Algunos desafíos que se deben tener en cuenta son:

**Incompatibilidades entre Librerías:** Si cada equipo usa versiones diferentes de las mismas librerías, pueden surgir conflictos que afecten el rendimiento.

 - Carga Simultánea: Si no se gestiona bien la carga de varios microfrontends, la aplicación puede volverse lenta.
 - Coordinación entre Equipos: La independencia tecnológica puede ser un arma de doble filo. Sin una buena comunicación, la integración final puede complicarse.


## **Ventajas y Desventajas de los Microfrontends**

## **Ventajas**

 - Desarrollo en Paralelo: Los equipos pueden trabajar en diferentes módulos sin depender unos de otros.
 - Menor Riesgo: Un error en un microfrontend no afecta a toda la aplicación.
 - Facilidad de Mantenimiento: Cada módulo puede evolucionar por separado, lo que facilita las actualizaciones.
   
## **Desventajas**

 - Complejidad en la Integración: A veces, coordinar varios microfrontends puede ser complicado.
 - Sobrecarga Operativa: La gestión de múltiples módulos requiere más recursos, tanto humanos como tecnológicos.

# **Demo de codigo sobre Microfrontends**

Para demostrar las ventajas y la funcionalidad general de los microfrontends, se elabora un caso de uso simple que nos permitira visualizar especificamente este proceso.

En esta instancia, el caso de uso sera una aplicacion web sencilla que contiene dos modulos principales: 
**`header-app`**: Proporciona un componente de encabezado (`Header`) que es expuesto para ser utilizado remotamente.
**`home-app`**: Consume el encabezado desde `header-app` y muestra contenido adicional (imágenes aleatorias).


El objetivo de este simple demo es visualizar como se puede comunicar dos modulos o microfront ends entre si mediante Webpack Moduloe Federation

## Requisitos previos

- **Node.js** (versión 14 o superior)
- **npm** (instalado con Node.js)

## Instalación

### 1. Crear las aplicaciones

#### Crear `header-app`

```
npx create-react-app header-app
cd header-app
```

Se crea home-app

```
npx create-react-app home-app
cd home-app
```
## 2. Instalar dependencias necesarias para Webpack y Module Federation

En ambas aplicaciones, se deben instalar las dependencias necesarias para personalizar Webpack y habilitar Module Federation.

```
cd header-app
npm install webpack webpack-cli webpack-dev-server html-webpack-plugin @babel/core babel-loader @babel/preset-react --save-dev

```
```
cd home-app
npm install webpack webpack-cli webpack-dev-server html-webpack-plugin @babel/core babel-loader @babel/preset-react --save-dev

```

## 3. Configurar Webpack

Configura Webpack para que header-app exponga el componente Header:
```
const HtmlWebpackPlugin = require('html-webpack-plugin');
const ModuleFederationPlugin = require('webpack/lib/container/ModuleFederationPlugin');
const path = require('path');

module.exports = {
  entry: './src/index.js',
  mode: 'development',
  devServer: {
    port: 3000,  // Puerto donde correrá header-app
  },
  output: {
    publicPath: 'auto',
  },
  module: {
    rules: [
      {
        test: /\.js$/,
        use: 'babel-loader',
        exclude: /node_modules/,
      },
    ],
  },
  plugins: [
    new ModuleFederationPlugin({
      name: 'appHeader',
      filename: 'remoteEntry.js',
      exposes: {
        './Header': './src/Header',  // Exponer el componente Header
      },
      shared: { react: { singleton: true }, 'react-dom': { singleton: true } },
    }),
    new HtmlWebpackPlugin({
      template: './public/index.html',
    }),
  ],
};


```


Al mismo tiempo, se configura webpack en home-app para consumir el Header desde header-app
```
const HtmlWebpackPlugin = require('html-webpack-plugin');
const ModuleFederationPlugin = require('webpack/lib/container/ModuleFederationPlugin');
const path = require('path');

module.exports = {
  entry: './src/index.js',
  mode: 'development',
  devServer: {
    port: 3001,  // Puerto donde correrá home-app
  },
  output: {
    publicPath: 'auto',
  },
  module: {
    rules: [
      {
        test: /\.js$/,
        use: 'babel-loader',
        exclude: /node_modules/,
      },
    ],
  },
  plugins: [
    new ModuleFederationPlugin({
      name: 'appContent',
      remotes: {
        appHeader: 'appHeader@http://localhost:3000/remoteEntry.js',  // Consumir el Header remoto
      },
      shared: { react: { singleton: true }, 'react-dom': { singleton: true } },
    }),
    new HtmlWebpackPlugin({
      template: './public/index.html',
    }),
  ],
};

```

En esta ocasion, se cambia el App.js en cada modulo por un nombre unico para diferenciar, Header.js y Content.js

### Header.js
```
import React from 'react';

const Header = () => {
  return (
    <header style={{ padding: '20px', backgroundColor: '#282c34', color: 'white', textAlign: 'center' }}>
      <h1>Microfrontend Header</h1>
    </header>
  );
};

export default Header;
```
### Content.js (Contiene un fetch para sacar imagenes aleatorias de una pagina)
```
import React, { useState, useEffect } from 'react';

const Content = () => {
  const [image, setImage] = useState('');

  const fetchRandomImage = async () => {
    const res = await fetch('https://source.unsplash.com/random');
    setImage(res.url);
  };

  useEffect(() => {
    fetchRandomImage();
  }, []);

  return (
    <div style={{ textAlign: 'center' }}>
      <h2>Random Image</h2>
      <img src={image} alt="Random" style={{ width: '80%', maxWidth: '600px', height: 'auto' }} />
      <br />
      <button onClick={fetchRandomImage}>Get Another Image</button>
    </div>
  );
};

export default Content;

```

Se integra el index.js, el cual renderiza el componente Header.js remoto y el contenido en el app o modulo home
```
import React from 'react';
import { createRoot } from 'react-dom/client';
import Content from './Content';

// Importar el Header remoto desde header-app
const Header = React.lazy(() => import('appHeader/Header'));

const App = () => (
  <React.Suspense fallback={<div>Loading Header...</div>}>
    <Header />
    <Content />
  </React.Suspense>
);

const container = document.getElementById('root');
const root = createRoot(container);
root.render(<App />);

```

Una vez se siga estos pasos, se puede probar mediante 

```
npm start
```
**Primero se empieza el aplicativo Header en el puerto 3000**
![headerimagen](https://github.com/user-attachments/assets/9b3c31cf-7843-4788-994c-655cf631a9b3)

Luego, se puede empezar el app Home el cual consume el app de cabecera

![contentimage](https://github.com/user-attachments/assets/10c5904d-fa11-43da-8824-253f520dfa1c)


Este ejercicio de microfrontends utilizando Module Federation nos ha mostrado cómo podemos dividir una aplicación en componentes independientes que interactúan entre sí sin estar fuertemente acoplados. Hemos visto cómo header-app proporciona un componente específico (el encabezado), mientras que home-app lo consume y lo integra con su propio contenido, en este caso, mostrando imágenes aleatorias.

La clave de los microfrontends es que cada parte de la aplicación puede ser desarrollada, desplegada y actualizada de forma independiente, lo que mejora la escalabilidad y flexibilidad de nuestros proyectos. Esto permite a equipos trabajar en diferentes secciones de una aplicación sin generar conflictos y con un impacto mínimo en el desarrollo general.

**Video de expocision**: [Link del Video](https://drive.google.com/file/d/1gzYzUvk2MzXi9WTIrh-9DL_9sUo8EqXO/view?usp=sharing)


Referencias:

https://webpack.js.org/concepts/module-federation/

https://www.plainconcepts.com/es/micro-frontends/

https://micro-frontends-es.org/

https://es.single-spa.js.org/docs/microfrontends-concept/

https://webpack.js.org/concepts/module-federation/



# Cloud Pattern: Bulkhead

##  Problema:
- Descripción del Problema: El patrón Bulkhead aborda el riesgo de que la falla de un componente en una aplicación afecte negativamente a otros componentes, causando un fallo en cascada. Sin una separación adecuada, un aumento en la carga de trabajo o un error en una sección de la aplicación puede impactar en todo el sistema, reduciendo la disponibilidad general de la aplicación y deteriorando la experiencia del usuario.


##  Solución:
- Descripción de la Solución: La solución Bulkhead consiste en dividir una aplicación en compartimentos (o “bulkheads”) de tal manera que cada sección funcione en un entorno aislado. Esto permite que, si un compartimento falla o se satura, el resto de los compartimentos y sus funciones continúen operando sin interrupciones. En términos prácticos, puede lograrse a través de la segmentación en contenedores, limitación de recursos, o configuraciones específicas que garanticen la independencia de cada componente.


- Información Complementaria: Además de lo mencionado en el catálogo de Microsoft, en plataformas de contenedores como Kubernetes, el patrón Bulkhead puede aplicarse mediante "pods" separados para diferentes servicios, con recursos específicos asignados a cada uno. También, en un entorno de nube, el uso de límites de conexión en servicios como bases de datos o colas de mensajes es una forma de evitar que una sobrecarga en un servicio afecte a los demás.


##  Casos de Aplicación:
- Banca y Finanzas: En servicios de banca en línea, los sistemas de pago, transferencias y consultas de cuentas pueden estar compartimentados. Esto asegura que si el servicio de consulta de saldo experimenta una sobrecarga, los pagos y transferencias siguen funcionando de forma independiente.
- Servicios de Streaming: En una plataforma de streaming, se pueden compartimentar los servicios de transmisión, recomendaciones y gestión de cuentas. De este modo, si hay un fallo en la carga de recomendaciones, la transmisión de contenido continúa sin interrupciones.
- Sector Retail/E-commerce: En un sitio de e-commerce, se pueden separar las funciones de búsqueda de productos, carrito de compras y procesamiento de pagos. Así, si la búsqueda de productos se satura durante promociones, los clientes aún pueden completar sus compras y procesar pagos.


##  Aplicación en el Trabajo de Grupo:
- Posible Aplicación: En el proyecto grupal, el patrón Bulkhead podría aplicarse para mantener la estabilidad de distintos módulos de la aplicación. Dado a que la aplicacion cuenta con módulos de autenticación, procesamiento de datos y mensajería, cada uno podría estar compartimentado. Esto asegura que si hay problemas en la mensajería, los usuarios aún puedan autenticarse y trabajar en el procesamiento de datos.


- Beneficios y Consideraciones:
  - Beneficios: Al implementar Bulkhead, se mejora la resiliencia y disponibilidad de la aplicación, ya que cada módulo funciona de manera autónoma. Esto permite que el sistema sea más tolerante a fallos y facilita la administración de recursos de manera más precisa.

- Consideraciones: Es importante evaluar el costo y la infraestructura necesaria para implementar la compartimentación, ya que puede requerir contenedores o servicios separados con administración individual. También es crucial diseñar un sistema de monitoreo para cada compartimento, de modo que los problemas puedan identificarse rápidamente sin impactar al resto del sistema.

# Demo para Bulkhead

## Caso de Uso Real: Plataforma de Streaming
Contexto: Para el demo se plantea un caso de plataforma de streaming que ofrece contenido en video a usuarios. Esta plataforma tiene varios servicios críticos que deben funcionar independientemente para garantizar una buena experiencia de usuario:

Servicio de Transmisión de Video: Responsable de la transmisión en vivo y contenido bajo demanda.
Servicio de Recomendaciones: Genera sugerencias de contenido personalizado en función del historial de visualización.
Servicio de Gestión de Usuarios: Administra información de usuario, como suscripciones y preferencias.


Cada uno de estos servicios debe funcionar de forma independiente para evitar que los problemas en un servicio afecten a los otros. Por ejemplo, si el servicio de recomendaciones está temporalmente fuera de servicio, los usuarios aún deben poder ver videos y gestionar sus cuentas

Para el codigo, se usa un demo simple en Python con hilos para crear compartimentos independientes (bulkheads) para cada servicio.


```
import threading
import time
import random

# simulacion bulkhead con hilos

def servicio_transmision():
    print("[Bulkhead Transmisión] Iniciando transmisión de video...")
    time.sleep(random.uniform(2, 5))  # Simula tiempo de procesamiento
    print("[Bulkhead Transmisión] Transmisión de video completada.")

def servicio_recomendaciones():
    print("[Bulkhead Recomendaciones] Generando recomendaciones...")
    time.sleep(random.uniform(1, 4))  # Simula tiempo de procesamiento
    print("[Bulkhead Recomendaciones] Recomendaciones generadas.")

def servicio_gestion_usuarios():
    print("[Bulkhead Gestión de Usuarios] Actualizando preferencias de usuario...")
    time.sleep(random.uniform(1, 3))  # Simula tiempo de procesamiento
    print("[Bulkhead Gestión de Usuarios] Preferencias de usuario actualizadas.")

# Definir los bulkheads como hilos independientes
def bulkhead_servicios():
    # Crear hilos para cada servicio
    bulkhead_transmision = threading.Thread(target=servicio_transmision)
    bulkhead_recomendaciones = threading.Thread(target=servicio_recomendaciones)
    bulkhead_gestion = threading.Thread(target=servicio_gestion_usuarios)

    # Iniciar los hilos
    bulkhead_transmision.start()
    bulkhead_recomendaciones.start()
    bulkhead_gestion.start()

    # Esperar a que cada hilo complete su ejecución
    bulkhead_transmision.join()
    bulkhead_recomendaciones.join()
    bulkhead_gestion.join()

if __name__ == "__main__":
    print("Iniciando demo del patrón Bulkhead para plataforma de streaming...\n")
    bulkhead_servicios()
    print("\nDemo completada.")

```

En mas detalle, cada función creada en el codigo representa un servicio de la plataforma streaming. Cada uno tiene una operación que tarda un tiempo variable en completarse, simulando el procesamiento real de la aplicación.

- Separación en Bulkheads: Utilizamos hilos para que cada servicio funcione de forma independiente. Así, si uno de ellos tarda mucho o falla, los otros continuarán funcionando.
- Ejecución en Paralelo: Los hilos permiten que los servicios operen en paralelo, asegurando la independencia de cada compartimento.

Output: El resultado de este ejercicio simple se puede visualizar en el terminal al correr el codigo.


![image](https://github.com/user-attachments/assets/27534cd6-d65d-46de-ac06-7741029b8001)


Este ejemplo simula una plataforma de streaming en la que cada servicio opera de forma independiente, garantizando que un problema en uno de ellos no afecte la disponibilidad de los otros y la aplicacion siga corriendo.

El proceso de implementacion seria el siguiente: 

- Definición del Problema: Los servicios en la plataforma de streaming deben continuar operando de forma independiente para evitar que problemas en un servicio - afecten a los servicios críticos (como Transmisión de Video).
- Aislamiento con Bulkhead: Usamos hilos para separar cada servicio en su propio "compartimento", lo que permite que los recursos se gestionen de forma aislada y cada servicio funcione sin depender de los demás.
- Ejecución en Paralelo: Se emplea el módulo threading para ejecutar los servicios en paralelo y evitar fallos en cascada, probando así la efectividad del patrón Bulkhead en una situación real de independencia de servicios.

  
ARCHIVOS SE ENCUENTRAN EN BRANCH "MASTER", NO LA MAIN
Link a repositorio utilizado para demo: [Repositorio](https://github.com/JorgeJusto/demoBulkhead.git)



# Caso Real de plataforma Streaming

El demo presentado previamente es un ejemplo simple de la premisa y objetivo de Bulkhead, en mas detalle, si se quisiera aplicar estos conceptos a una plataforma de Streaming de alto nivel, se podrian tomar pasos como los siguientes para su implementacion: 

## 1. Definición de los Servicios Críticos y Sus Prioridades

- Identificar Servicios Críticos: En una plataforma de streaming, los servicios que deben funcionar de manera aislada pueden incluir:
Transmisión de Video: Responsable de la entrega de contenido multimedia a los usuarios en tiempo real o bajo demanda.
- Sistema de Recomendaciones: Genera recomendaciones personalizadas de contenido para cada usuario.
- Gestión de Usuarios: Administra autenticación, suscripciones, y configuración del usuario.
- Publicidad (si es que aplica): Gestiona la inserción de anuncios en el contenido.
- Asignar Prioridades: En este caso, la transmisión de video tendría la mayor prioridad, seguida de la gestión de usuarios. El sistema de recomendaciones y la publicidad pueden tener menor prioridad, ya que no afectan directamente la experiencia de visualización.


## 2. Aislamiento de los Recursos en Contenedores o Microservicios

- Contenedores y Microservicios: Cada servicio identificado debe implementarse en contenedores o microservicios separados para garantizar su aislamiento. Por ejemplo, el servicio de transmisión de video podría ejecutarse en un grupo de contenedores optimizado para la entrega rápida de contenido, mientras que el servicio de recomendaciones podría utilizar un grupo separado que consuma menos recursos y pueda ser escalado de manera independiente.
- Infraestructura en la Nube: Utilizar una infraestructura en la nube (como AWS, Azure, o Google Cloud) para implementar estos microservicios permite configurar recursos de forma independiente. Por ejemplo, Kubernetes o Docker Swarm se pueden usar para orquestar y aislar estos contenedores en diferentes “pods” o “grupos”.


## 3. Configuración de Límites de Recursos

- Control de Consumo de CPU y Memoria: Configura límites estrictos de consumo de CPU y memoria para cada servicio de manera independiente. Por ejemplo, asigna más CPU y memoria al servicio de transmisión de video y menos al servicio de recomendaciones. Esto asegura que, si el servicio de recomendaciones experimenta un aumento en la demanda, no afecte la disponibilidad de recursos para la transmisión de video.
- Autoscaling: Configura reglas de autoscaling (escalado automático) para que cada servicio pueda aumentar o reducir los recursos según la demanda. Para la transmisión de video, esto es fundamental en eventos de alta demanda, como lanzamientos de contenido popular.

## 4. Implementación de Mecanismos de Recuperación y Monitoreo
- Mecanismos de Recuperación: Configura políticas de recuperación automática (como el reinicio de contenedores) para los servicios. Si un servicio específico, como el de recomendaciones, falla, debe reiniciarse sin impactar otros servicios. Las plataformas en la nube permiten definir estas reglas de manera individual para cada contenedor o microservicio.
- Monitoreo de Compartimentos: Utiliza herramientas de monitoreo como Prometheus o servicios de monitoreo en la nube (por ejemplo, AWS CloudWatch o Azure Monitor) para rastrear la salud y el rendimiento de cada compartimento. Configura alertas específicas para cada servicio, de modo que si el servicio de transmisión de video muestra latencia, se pueda escalar rápidamente sin afectar los otros servicios.

## 5. Implementación de Políticas de Fallback o Degradación Controlada

- Fallback para Servicios No Críticos: Implementa estrategias de fallback para servicios secundarios. Por ejemplo, si el sistema de recomendaciones no está disponible, el sistema puede mostrar contenido popular en lugar de sugerencias personalizadas.
- Degradación Controlada: En situaciones de alta demanda, permite que servicios de menor prioridad (como recomendaciones y publicidad) se degraden primero, garantizando así que el servicio de transmisión de video mantenga su calidad. Esto puede implicar reducir la frecuencia de llamadas a ciertos servicios o cambiar a modos de servicio más ligeros.

## 6. Aislamiento de las Bases de Datos

- Bases de Datos Separadas: Para garantizar que la carga de un servicio no afecte a otros, cada servicio puede tener su propia base de datos o acceso independiente a una base de datos compartida mediante cuotas o particiones. Esto evita que un aumento en las consultas del sistema de recomendaciones, por ejemplo, impacte la disponibilidad de la base de datos de usuarios.
- Optimización de Consultas: Configura cachés independientes para cada servicio o usa un almacenamiento en caché distribuido para optimizar el acceso a datos frecuentemente solicitados sin recargar las bases de datos principales.

## 7. Pruebas de Resiliencia y Escalabilidad

- Pruebas de Carga y Resiliencia: Realiza pruebas para cada servicio de manera independiente y combinada, simulando altos volúmenes de tráfico en unos servicios y bajas en otros. Esto permitirá identificar cualquier punto débil en la implementación del patrón Bulkhead.
- Análisis de Fallos: Verifica cómo responde cada compartimento en situaciones de fallo. Por ejemplo, simula un fallo en el servicio de recomendaciones y verifica que la transmisión de video y la gestión de usuarios siguen funcionando sin problemas.

-------------------------------------------------------

# Segundo Tema Individual: Benchmarking de LLM para Generación de Código

Hoy en día, los modelos de lenguaje grandes (LLMs) han revolucionado la forma en que se abordan tareas de programación, desde la generación de código hasta la depuración y optimización. Benchmarking en este contexto implica evaluar de manera sistemática y comparativa el rendimiento de los LLMs en tareas específicas de generación de código. Esto permite identificar sus fortalezas, limitaciones y aplicaciones prácticas, proporcionando una base sólida para elegir el modelo adecuado para necesidades particulares.

## Aplicación del Benchmarking de LLM en la Vida Real
Un ejemplo práctico sería evaluar cómo un LLM como GPT-4 o Codex maneja tareas de programación en un entorno empresarial. Por ejemplo, si una empresa necesita automatizar la generación de funciones para calcular métricas estadísticas, el modelo podría ser evaluado en:

- Correctitud: El código generado calcula las métricas correctamente?
- Eficiencia: El código es óptimo en términos de recursos?
- Calidad del Estilo: Sigue las mejores prácticas de codificación?
  
Un caso similar es el desarrollo de herramientas internas de software. Un equipo de desarrollo podría usar el benchmarking para decidir entre varios LLMs al generar scripts automatizados o plantillas para proyectos repetitivos, ahorrando tiempo y reduciendo errores.

## Importancia del Benchmarking
- Selección Informada: Permite a las organizaciones identificar el modelo más adecuado para sus necesidades específicas.
- Medición de Capacidades: Evalúa qué tan bien un modelo entiende y genera código en diferentes lenguajes de programación.
- Optimización de Procesos: Facilita la implementación de modelos en flujos de trabajo existentes, asegurando una integración sin problemas.
- Identificación de Limitaciones: Descubre áreas donde los modelos necesitan mejoras, como manejar ambigüedades en las especificaciones.
## Claves para Realizar un Benchmarking Efectivo
## Métricas de Evaluación:

- Precisión: Exactitud del código generado.
- Ejecutabilidad: Proporción de código que se ejecuta correctamente.
- Legibilidad: Claridad y mantenimiento del código.
- Tiempos de Respuesta: Tiempo que toma el modelo en generar una solución.
- Adaptabilidad: Capacidad para ajustar la generación a diferentes estilos o lenguajes de programación.

# Algunos modelos disponibles para comparacion:

OpenAI Codex: Especializado en tareas de programación.
CodeT5: Optimizado para generación y transformación de código.
StarCoder: Diseñado para múltiples lenguajes de programación.

## Herramientas Comunes en Benchmarking
- Eval Harness: Framework para evaluar el rendimiento de LLMs en generación de código.
- APIs de Modelos LLM: Uso de OpenAI, Hugging Face o Cohere para pruebas comparativas.
Frameworks de Desarrollo: Streamlit o Flask para construir interfaces de prueba.

# Demo de modelos para generar codigo

Para este caso de uso, se utilizara algunos modelos open-source encontrados en la red para generar codigos utilizando el mismo prompt, luego, se realizara el benchmarking comparando el tiempo de ejecucion del modelo, la calidad del codigo, generado, etc.

## Modelos utilizados

- Code Parrot
- HuggingFaces
- Open-Source Copilot

